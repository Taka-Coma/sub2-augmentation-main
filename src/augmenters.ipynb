{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b54c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821f35a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eab8a227514a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import copy\n",
    "import random\n",
    "import regex\n",
    "from nltk import Tree\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from data import Sentence\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.original_dataset = dataset\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.dataset = copy.deepcopy(self.original_dataset)\n",
    "\n",
    "\n",
    "class POSTagAugmenter(Augmenter):\n",
    "    def __init__(self, dataset, n_gram=4):\n",
    "        super(POSTagAugmenter, self).__init__(dataset)\n",
    "        self.k = n_gram\n",
    "        self.build_ngram_table(dataset, n_gram)\n",
    "\n",
    "    def build_ngram_table(self, dataset, k):\n",
    "        self.ngram_table = collections.defaultdict(list)\n",
    "        for i, sent in enumerate(dataset):\n",
    "            assert len(sent.words) == len(sent.tags)\n",
    "            for j in range(k):\n",
    "                for s in range(len(sent.tags) - j):\n",
    "                    subseq_tag = '_'.join(sent.tags[s:s+j+1])\n",
    "                    self.ngram_table[subseq_tag].append((i, s, j+1))\n",
    "        self.ngram_positions = list()\n",
    "        for key in self.ngram_table:\n",
    "            self.ngram_positions.extend(self.ngram_table[key])\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset) < size:\n",
    "            position = random.randint(0, len(self.ngram_positions) - 1)\n",
    "            sent_id, start_id, length = self.ngram_positions[position]\n",
    "            subseq_tag = '_'.join(\n",
    "                self.dataset[sent_id].tags[start_id:start_id+length]\n",
    "            )\n",
    "            sub_position = random.randint(\n",
    "                0, len(self.ngram_table[subseq_tag]) - 1\n",
    "            )\n",
    "            sub_sent_id, sub_start_id, sub_length = \\\n",
    "                self.ngram_table[subseq_tag][sub_position]\n",
    "            assert sub_length == length\n",
    "            subseq_words = self.dataset[sent_id].words[\n",
    "                start_id:start_id+length]\n",
    "            sub_subseq_words = self.dataset[sub_sent_id].words[\n",
    "                sub_start_id:sub_start_id+sub_length]\n",
    "            if subseq_words == sub_subseq_words:\n",
    "                continue\n",
    "            sub_sentence = copy.deepcopy(self.dataset[sent_id])\n",
    "            sub_sentence.words = sub_sentence.words[:start_id] + \\\n",
    "                sub_subseq_words + \\\n",
    "                sub_sentence.words[start_id+length:]\n",
    "            self.dataset.data.append(sub_sentence)\n",
    "            bar.update()\n",
    "        return self.dataset\n",
    "        \n",
    "\n",
    "class CParseAugmenter(Augmenter):\n",
    "    def __init__(\n",
    "                self, dataset, span_min_length=4, span_max_length=20\n",
    "            ):\n",
    "        super(CParseAugmenter, self).__init__(dataset)\n",
    "        self.rg = (span_min_length, span_max_length)\n",
    "        self.build_subtree_table(dataset, self.rg)\n",
    "\n",
    "    @staticmethod\n",
    "    def collect_subtrees(tree, left=0):\n",
    "        if isinstance(tree, str):\n",
    "            return\n",
    "        current_left = left\n",
    "        for child in tree:\n",
    "            for item in CParseAugmenter.collect_subtrees(child, current_left):\n",
    "                yield item\n",
    "            child_len = 1 if isinstance(child, str) else len(child.leaves())\n",
    "            current_left += child_len\n",
    "        yield tree, left, len(tree.leaves())\n",
    "\n",
    "    def build_subtree_table(self, dataset, span_length_range):\n",
    "        self.subtree_table = collections.defaultdict(list)\n",
    "        self.all_subtrees = list()\n",
    "        span_min_length, span_max_length = span_length_range\n",
    "        for i, tree in enumerate(tqdm(dataset.trees)):\n",
    "            tree_info = self.collect_subtrees(tree)\n",
    "            for subtree, left, length in tree_info:\n",
    "                if length < span_min_length or length > span_max_length:\n",
    "                    continue\n",
    "                subtree_info = (i, left, length, copy.deepcopy(subtree))\n",
    "                self.subtree_table[subtree.label(), length].append(\n",
    "                    subtree_info\n",
    "                )\n",
    "                self.all_subtrees.append(subtree_info)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset.trees) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset.trees)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            position = random.randint(0, len(self.all_subtrees) - 1)\n",
    "            idx, left, length, subtree = self.all_subtrees[position]\n",
    "            sub_position = random.randint(\n",
    "                0, len(self.subtree_table[subtree.label(), length]) - 1\n",
    "            )\n",
    "            sub_idx, sub_left, sub_length, sub_subtree = self.subtree_table[\n",
    "                subtree.label(), length][sub_position]\n",
    "            assert sub_length == length\n",
    "            if ' '.join(sub_subtree.leaves()) == ' '.join(subtree.leaves()):\n",
    "                continue\n",
    "            try:\n",
    "                new_tree = self.substitute_tree(\n",
    "                    self.dataset.trees[idx], 0, left, length, sub_subtree\n",
    "                )\n",
    "            except:\n",
    "                print('erorr performing substitution.')\n",
    "                from IPython import embed; embed(using=False)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def substitute_tree(tree, left, goal_left, length, subtree):\n",
    "        tree_label = None if isinstance(tree, str) else tree.label()\n",
    "        len_tree = 1 if isinstance(tree, str) else len(tree.leaves())\n",
    "        if left == goal_left and len_tree == length and \\\n",
    "                tree_label == subtree.label():\n",
    "            return copy.deepcopy(subtree)\n",
    "        elif isinstance(tree, str):\n",
    "            return tree\n",
    "        elif left > goal_left:\n",
    "            return copy.deepcopy(tree)\n",
    "        current_left = left\n",
    "        new_children = list()\n",
    "        for child in tree:\n",
    "            new_child = CParseAugmenter.substitute_tree(\n",
    "                child, current_left, goal_left, length, subtree\n",
    "            )\n",
    "            new_children.append(new_child)\n",
    "            len_child = 1 if isinstance(child, str) else len(child.leaves())\n",
    "            current_left += len_child\n",
    "        return Tree(tree.label(), new_children)\n",
    "\n",
    "    @staticmethod\n",
    "    def rebuild(tree, words):\n",
    "        if isinstance(tree, str):\n",
    "            assert len(words) == 1\n",
    "            return words[0]\n",
    "        assert len(tree.leaves()) == len(words)\n",
    "        left = 0\n",
    "        new_children = list()\n",
    "        for child in tree:\n",
    "            n_leaves = len(child.leaves()) if isinstance(child, Tree) else 1\n",
    "            new_child = CParseAugmenter.rebuild(\n",
    "                child, words[left:left+n_leaves]\n",
    "            )\n",
    "            left += n_leaves\n",
    "            new_children.append(new_child)\n",
    "        return Tree(tree.label(), new_children)\n",
    "\n",
    "\n",
    "class CParseLengthFreeAugmenter(CParseAugmenter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CParseLengthFreeAugmenter, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build_subtree_table(self, dataset, span_length_range):\n",
    "        self.subtree_table = collections.defaultdict(list)\n",
    "        self.all_subtrees = list()\n",
    "        span_min_length, span_max_length = span_length_range\n",
    "        for i, tree in enumerate(tqdm(dataset.trees)):\n",
    "            tree_info = self.collect_subtrees(tree)\n",
    "            for subtree, left, length in tree_info:\n",
    "                if length < span_min_length or length > span_max_length:\n",
    "                    continue\n",
    "                subtree_info = (i, left, length, copy.deepcopy(subtree))\n",
    "                self.subtree_table[subtree.label()].append(subtree_info)\n",
    "                self.all_subtrees.append(subtree_info)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset.trees) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset.trees)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            position = random.randint(0, len(self.all_subtrees) - 1)\n",
    "            idx, left, length, subtree = self.all_subtrees[position]\n",
    "            sub_position = random.randint(\n",
    "                0, len(self.subtree_table[subtree.label()]) - 1\n",
    "            )\n",
    "            sub_idx, sub_left, sub_length, sub_subtree = self.subtree_table[\n",
    "                subtree.label()][sub_position]\n",
    "            assert subtree.label() == sub_subtree.label()\n",
    "            if ' '.join(sub_subtree.leaves()) == ' '.join(subtree.leaves()):\n",
    "                continue\n",
    "            try:\n",
    "                new_tree = self.substitute_tree(\n",
    "                    self.dataset.trees[idx], 0, left, length, sub_subtree\n",
    "                )\n",
    "            except:\n",
    "                print('erorr performing substitution.')\n",
    "                from IPython import embed; embed(using=False)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class CParseSynonymAugmenter(CParseAugmenter):\n",
    "    def __init__(\n",
    "                self, dataset, \n",
    "                language_model='bert-large-cased-whole-word-masking'\n",
    "            ):\n",
    "        super(CParseSynonymAugmenter, self).__init__(dataset)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(language_model).cpu()\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            tree_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_tree = copy.deepcopy(self.dataset.trees[tree_id])\n",
    "            words = list(sub_tree.leaves())\n",
    "            position = random.randint(0, len(words)-1)\n",
    "            orig_token = words[position]\n",
    "            # only perform SUB on word tokens\n",
    "            if not regex.match('.*[a-z].*', orig_token):\n",
    "                continue\n",
    "            # compute substitution and output a different sub candidate\n",
    "            words[position] = self.tokenizer.mask_token\n",
    "            inputs = self.tokenizer(' '.join(words), return_tensors='pt')\n",
    "            subtoken_position = (\n",
    "                inputs['input_ids'][0] == self.tokenizer.mask_token_id\n",
    "            ).long().argmax().item()\n",
    "            labels = inputs['input_ids']\n",
    "            outputs = self.model(**inputs, labels=labels)[1][0]\n",
    "            while True:\n",
    "                subtoken_id = outputs[subtoken_position].argmax().item()\n",
    "                subtoken = self.tokenizer.convert_ids_to_tokens(subtoken_id)\n",
    "                if subtoken != orig_token:\n",
    "                    break\n",
    "                else:\n",
    "                    outputs[subtoken_position][subtoken_id] = -1e10\n",
    "            words[position] = subtoken\n",
    "            new_tree = self.rebuild(sub_tree, words)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class CParseRandomAugmenter(CParseAugmenter):\n",
    "    def __init__(self, dataset):\n",
    "        super(CParseRandomAugmenter, self).__init__(dataset)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            tree_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_tree = copy.deepcopy(self.dataset.trees[tree_id])\n",
    "            words = list(sub_tree.leaves())\n",
    "            random.shuffle(words)\n",
    "            new_tree = self.rebuild(sub_tree, words)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class CParseSpanAugmenter(CParseAugmenter):\n",
    "    def __init__(self, dataset):\n",
    "        super(CParseSpanAugmenter, self).__init__(dataset)\n",
    "        self.example_table = self.build_example_table()\n",
    "\n",
    "    def build_example_table(self):\n",
    "        example_table = collections.defaultdict(list)\n",
    "        for tree in self.dataset.trees:\n",
    "            label = tree.label()\n",
    "            example_table[label].append(tree)\n",
    "        return example_table\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            tree_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_tree = copy.deepcopy(self.dataset.trees[tree_id])\n",
    "            label = sub_tree.label()\n",
    "            words = list(sub_tree.leaves())\n",
    "            print(\"##########\")\n",
    "            print(words)\n",
    "\n",
    "\n",
    "\n",
    "            sub_left = random.randint(0, len(words) - 1)\n",
    "            sub_right = random.randint(sub_left + 1, len(words))\n",
    "            candidate_id = random.randint(0, len(self.example_table[label]) - 1)\n",
    "            candidate_tree = copy.deepcopy(self.example_table[label][candidate_id])\n",
    "            candidate_words = candidate_tree.leaves()\n",
    "            can_left = random.randint(0, len(candidate_words) - 1)\n",
    "            can_right = random.randint(can_left + 1, len(candidate_words))\n",
    "            new_words = words[:sub_left] + candidate_words[can_left:can_right] + words[sub_right:]\n",
    "            new_tree = Tree(label, new_words)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class CParseLengthSpanAugmenter(CParseAugmenter):\n",
    "    def __init__(self, dataset):\n",
    "        super(CParseLengthSpanAugmenter, self).__init__(dataset)\n",
    "        self.example_table = self.build_example_table()\n",
    "\n",
    "    def build_example_table(self):\n",
    "        example_table = collections.defaultdict(list)\n",
    "        for tree in self.dataset.trees:\n",
    "            label = tree.label()\n",
    "            example_table[label].append(tree)\n",
    "        return example_table\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        original_tree_size = len(self.dataset.trees)\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset.trees) < size:\n",
    "            tree_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_tree = copy.deepcopy(self.dataset.trees[tree_id])\n",
    "            label = sub_tree.label()\n",
    "            words = list(sub_tree.leaves())\n",
    "            sub_left = random.randint(0, len(words) - 1)\n",
    "            sub_right = random.randint(sub_left + 1, len(words))\n",
    "            candidate_id = random.randint(0, len(self.example_table[label]) - 1)\n",
    "            candidate_tree = copy.deepcopy(self.example_table[label][candidate_id])\n",
    "            candidate_words = candidate_tree.leaves()\n",
    "            can_left = random.randint(0, len(candidate_words) - 1)\n",
    "            can_right = can_left + sub_right - sub_left \n",
    "            if can_right > len(candidate_words):\n",
    "                continue\n",
    "            new_words = words[:sub_left] + candidate_words[can_left:can_right] + words[sub_right:]\n",
    "            new_tree = Tree(label, new_words)\n",
    "            self.dataset.trees.append(new_tree)\n",
    "            bar.update()\n",
    "        self.dataset.add_spans(self.dataset.trees[original_tree_size:])\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class DependencyParsingAugmenter(Augmenter):\n",
    "    def __init__(self, dataset, n_gram=4):\n",
    "        super(DependencyParsingAugmenter, self).__init__(dataset)\n",
    "        self.k = n_gram\n",
    "        self.build_subtree_table(dataset)\n",
    "\n",
    "    def build_subtree_table(self, dataset):\n",
    "        # augment with length restriction\n",
    "        # TODO (freda) consider removing the restriction? \n",
    "        self.subtrees = collections.defaultdict(list)\n",
    "        self.tree_infos = list()\n",
    "        self.subtree_list = list()\n",
    "        for i, sent in enumerate(dataset):\n",
    "            # drop if sentence has X-edges\n",
    "            x_edge = False\n",
    "            for k, j in enumerate(sent.ids):\n",
    "                assert k == j - 1\n",
    "                parent = sent.deps[k]\n",
    "                if parent == 0:\n",
    "                    continue\n",
    "                left, right = min(j, parent), max(j, parent)\n",
    "                for id_p in range(left + 1, right):\n",
    "                    parent_p = sent.deps[id_p - 1]\n",
    "                    if parent_p < left or parent_p > right:\n",
    "                        x_edge = True\n",
    "                if x_edge:\n",
    "                    break\n",
    "            if x_edge:\n",
    "                continue\n",
    "            # collect info \n",
    "            tree_info = collections.defaultdict(dict)\n",
    "            for j in sent.ids:\n",
    "                tree_info[j]['left'] = j\n",
    "                tree_info[j]['right'] = j\n",
    "            tree_info[0] = {'left': 1e10, 'right': 0}\n",
    "            for k, j in enumerate(sent.ids):\n",
    "                parent = sent.deps[k]\n",
    "                tree_info[parent]['left'] = min(\n",
    "                    tree_info[j]['left'], tree_info[parent]['left']\n",
    "                )\n",
    "            for k, j in reversed(list(enumerate(sent.ids))):\n",
    "                parent = sent.deps[k]\n",
    "                tree_info[parent]['right'] = max(\n",
    "                    tree_info[j]['right'], tree_info[parent]['right']\n",
    "                )\n",
    "            for k, j in enumerate(sent.ids):\n",
    "                length = tree_info[j]['right'] - tree_info[j]['left'] + 1\n",
    "                if length <= 1:\n",
    "                    continue\n",
    "                label = sent.dep_labels[k]\n",
    "                self.subtrees[length, label].append(\n",
    "                    [i, k, tree_info[j]['left']-1, tree_info[j]['right']]\n",
    "                )\n",
    "            self.tree_infos.append(tree_info)\n",
    "        for key in self.subtrees:\n",
    "            for subtree in self.subtrees[key]:\n",
    "                self.subtree_list.append(subtree + list(key))\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        # self.ids, self.words, self.tags, self.deps, self.dep_labels\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset) < size:\n",
    "            position = random.randint(0, len(self.subtree_list) - 1)\n",
    "            original_subtree = self.subtree_list[position]\n",
    "            length = original_subtree[-2]\n",
    "            label = original_subtree[-1]\n",
    "            substitution_position = random.randint(\n",
    "                0, len(self.subtrees[length, label]) - 1\n",
    "            )\n",
    "            substitution_subtree = self.subtrees[\n",
    "                length, label][substitution_position]\n",
    "            if substitution_subtree == original_subtree[:4]:\n",
    "                continue\n",
    "            ids = list(self.dataset[original_subtree[0]].ids)\n",
    "            words = list(self.dataset[original_subtree[0]].words)\n",
    "            tags = list(self.dataset[original_subtree[0]].tags)\n",
    "            deps = list(self.dataset[original_subtree[0]].deps)\n",
    "            dep_labels = list(self.dataset[original_subtree[0]].dep_labels)\n",
    "            orig_range = original_subtree[2:4]\n",
    "            sub_range = substitution_subtree[2:4]\n",
    "            words[orig_range[0]:orig_range[1]] = copy.deepcopy(\n",
    "                self.dataset[substitution_subtree[0]].words[\n",
    "                    sub_range[0]:sub_range[1]\n",
    "                ]\n",
    "            )\n",
    "            tags[orig_range[0]:orig_range[1]] = copy.deepcopy(\n",
    "                self.dataset[substitution_subtree[0]].tags[\n",
    "                    sub_range[0]:sub_range[1]\n",
    "                ]\n",
    "            )\n",
    "            orig_parent = self.dataset[original_subtree[0]].deps[\n",
    "                original_subtree[1]\n",
    "            ]\n",
    "            sub_deps = [\n",
    "                x - sub_range[0] for x in self.dataset[\n",
    "                    substitution_subtree[0]].deps[sub_range[0]:sub_range[1]]\n",
    "            ]\n",
    "            deps[orig_range[0]:orig_range[1]] = [\n",
    "                x + orig_range[0] for x in sub_deps\n",
    "            ]\n",
    "            deps[substitution_subtree[1] - sub_range[0] + orig_range[0]] = \\\n",
    "                orig_parent\n",
    "            dep_labels[orig_range[0]:orig_range[1]] = copy.deepcopy(\n",
    "                self.dataset[substitution_subtree[0]].dep_labels[\n",
    "                    sub_range[0]:sub_range[1]\n",
    "                ]\n",
    "            )\n",
    "            new_sent = Sentence.from_info(ids, words, tags, deps, dep_labels)\n",
    "            self.dataset.data.append(new_sent)\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class POSTagBaselineAugmenter(Augmenter):\n",
    "    def __init__(self, dataset, \n",
    "            language_model='bert-large-cased-whole-word-masking'):\n",
    "        super(POSTagBaselineAugmenter, self).__init__(dataset)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(language_model)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset) < size:\n",
    "            sent_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_sentence = copy.deepcopy(self.dataset[sent_id])\n",
    "            words = list(sub_sentence.words)\n",
    "            position = random.randint(0, len(words)-1)\n",
    "            orig_token = words[position]\n",
    "            # only perform SUB on word tokens\n",
    "            if not regex.match('.*[a-z].*', orig_token):\n",
    "                continue\n",
    "            # compute substitution and output a different sub candidate\n",
    "            words[position] = self.tokenizer.mask_token\n",
    "            inputs = self.tokenizer(' '.join(words), return_tensors='pt')\n",
    "            subtoken_position = (\n",
    "                inputs['input_ids'][0] == self.tokenizer.mask_token_id\n",
    "            ).long().argmax().item()\n",
    "            labels = inputs['input_ids']\n",
    "            outputs = self.model(**inputs, labels=labels)[1][0]\n",
    "            while True:\n",
    "                subtoken_id = outputs[subtoken_position].argmax().item()\n",
    "                subtoken = self.tokenizer.convert_ids_to_tokens(subtoken_id)\n",
    "                if subtoken != orig_token:\n",
    "                    break\n",
    "                else:\n",
    "                    outputs[subtoken_position][subtoken_id] = -1e10\n",
    "            words[position] = subtoken\n",
    "            sub_sentence.words = tuple(words)\n",
    "            self.dataset.data.append(sub_sentence)\n",
    "            bar.update()\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class DependencyParsingBaselineAugmenter(POSTagBaselineAugmenter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DependencyParsingBaselineAugmenter, self).__init__(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "class POSTagJointAugmenter(Augmenter):\n",
    "    def __init__(self, dataset):\n",
    "        super(POSTagJointAugmenter, self).__init__(dataset)\n",
    "        self.sub_augmenter = POSTagAugmenter(dataset)\n",
    "        self.synonym_augmenter = POSTagBaselineAugmenter(dataset)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        size_per_method = len(self.dataset) + (size - len(self.dataset)) // 2\n",
    "        sub_augs = self.sub_augmenter.augment(size_per_method)\n",
    "        synonym_augs = self.synonym_augmenter.augment(size_per_method)\n",
    "        self.dataset.data += sub_augs.data[len(self.dataset):] + \\\n",
    "            synonym_augs.data[len(self.dataset):]\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class DependencyParsingJointAugmenter(POSTagJointAugmenter):\n",
    "    def __init__(self, dataset):\n",
    "        super(DependencyParsingJointAugmenter, self).__init__(dataset)\n",
    "        self.sub_augmenter = DependencyParsingAugmenter(dataset)\n",
    "        self.synonym_augmenter = DependencyParsingBaselineAugmenter(dataset)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        size_per_method = len(self.dataset) + (size - len(self.dataset)) // 2\n",
    "        sub_augs = self.sub_augmenter.augment(size_per_method)\n",
    "        synonym_augs = self.synonym_augmenter.augment(size - size_per_method)\n",
    "        self.dataset.data += sub_augs.data[len(self.dataset):] + \\\n",
    "            synonym_augs.data[len(self.dataset):]\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class POSTagRandomAugmenter(Augmenter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(POSTagRandomAugmenter, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def augment(self, size=None):\n",
    "        if size is None:\n",
    "            size = len(self.dataset) * 2\n",
    "        bar = tqdm(range(size - len(self.dataset)))\n",
    "        bar.set_description(f'Running {type(self)}')\n",
    "        while len(self.dataset) < size:\n",
    "            sent_id = random.randint(0, len(self.dataset) - 1)\n",
    "            sub_sentence = copy.deepcopy(self.dataset[sent_id])\n",
    "            new2orig = [i for i in range(len(sub_sentence.words))]\n",
    "            random.shuffle(new2orig)\n",
    "            orig2new = {i: k for k, i in enumerate(new2orig)}\n",
    "            orig2new[-1] = -1  # root\n",
    "            pos = [sub_sentence.tags[i] for _, i in enumerate(new2orig)]\n",
    "            words = [sub_sentence.words[i] for _, i in enumerate(new2orig)]\n",
    "            deps = [\n",
    "                orig2new[sub_sentence.deps[i]-1]+1 \n",
    "                for _, i in enumerate(new2orig)\n",
    "            ]\n",
    "            dep_labels = [\n",
    "                sub_sentence.dep_labels[i] for _, i in enumerate(new2orig)\n",
    "            ]\n",
    "            sub_sentence.words = words\n",
    "            sub_sentence.tags = pos\n",
    "            sub_sentence.deps = deps\n",
    "            sub_sentence.dep_labels = dep_labels\n",
    "            self.dataset.data.append(sub_sentence)\n",
    "            bar.update()\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class DependencyParsingRandomAugmenter(POSTagRandomAugmenter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DependencyParsingRandomAugmenter, self).__init__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70933499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import PTBDataset\n",
    "\n",
    "random.seed(115)\n",
    "sst_dataset = PTBDataset(\n",
    "    # f'../data/sst/train_c.txt', use_spans=False\n",
    "    f'data/sst/train.txt', use_spans=False\n",
    ")\n",
    "sst_rand_augmenter = CParseRandomAugmenter(sst_dataset)\n",
    "sst_augmented_dataset = sst_rand_augmenter.augment(100000)\n",
    "with open('CParseRandom.binaryfile', 'wb') as f:\n",
    "    pickle.dump(sst_augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(115)\n",
    "sst_dataset = PTBDataset(\n",
    "    # f'../data/sst/train_c.txt', use_spans=False\n",
    "    f'data/sst/train.txt', use_spans=False\n",
    ")\n",
    "sst_syno_augmenter = CParseSynonymAugmenter(sst_dataset)\n",
    "sst_augmented_dataset = sst_syno_augmenter.augment(100000)\n",
    "with open('CParseSynonym.binaryfile', 'wb') as f:\n",
    "    pickle.dump(sst_augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(115)\n",
    "sst_dataset = PTBDataset(\n",
    "    # f'../data/sst/train_cl.txt', use_spans=True, span_min_length=4\n",
    "    f'data/sst/train.txt', use_spans=False\n",
    ")\n",
    "sst_free_augmenter = CParseLengthFreeAugmenter(sst_dataset)\n",
    "sst_augmented_dataset = sst_free_augmenter.augment(100000)\n",
    "with open('CParseLengthFree.binaryfile', 'wb') as f:\n",
    "    pickle.dump(sst_augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(115)\n",
    "sst_dataset = PTBDataset(\n",
    "    # f'../data/sst/train_cl.txt', use_spans=True, span_min_length=4\n",
    "    f'data/sst/train.txt', use_spans=False\n",
    ")\n",
    "sst_augmenter = CParseAugmenter(sst_dataset)\n",
    "sst_augmented_dataset = sst_augmenter.augment(100000)\n",
    "with open('CParse.binaryfile', 'wb') as f:\n",
    "    pickle.dump(sst_augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "# Joint augmenter unit test\n",
    "from data import UniversalDependenciesDataset\n",
    "dataset = UniversalDependenciesDataset(\n",
    "    # '../data/*/*/en*dev*conllu',\n",
    "    # '../data/universal-dependencies-1.2/tags.txt'\n",
    "    'data/conllu/en*dev*conllu',\n",
    "    'data/tags.txt'\n",
    ")\n",
    "augmenter = DependencyParsingJointAugmenter(dataset)\n",
    "augmented_dataset = augmenter.augment()\n",
    "with open('DependencyParsingJoint.binaryfile', 'wb') as f:\n",
    "    pickle.dump(augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "# POS tagging augmenter unit test\n",
    "from data import UniversalDependenciesDataset\n",
    "dataset = UniversalDependenciesDataset(\n",
    "    # '../data/*/*/en*dev*conllu',\n",
    "    # '../data/universal-dependencies-1.2/tags.txt'\n",
    "    'data/conllu/en*dev*conllu',\n",
    "    'data/tags.txt'\n",
    ")\n",
    "augmenter = POSTagBaselineAugmenter(dataset)\n",
    "augmented_dataset = augmenter.augment()\n",
    "with open('POSTagBaseline.binaryfile', 'wb') as f:\n",
    "    pickle.dump(augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "# Dep parsing unit test\n",
    "from data import UniversalDependenciesDataset\n",
    "dep_dataset = UniversalDependenciesDataset(\n",
    "    # '../data/universal_treebanks_v2.0/std/en/en-universal-dev.conll',\n",
    "    # '../data/universal_treebanks_v2.0/std/tags.txt'\n",
    "    'data/conllu/en-universal-dev.conll',\n",
    "    'data/tags.txt'\n",
    ")\n",
    "augmenter = DependencyParsingAugmenter(dep_dataset)\n",
    "augmented_dataset = augmenter.augment()\n",
    "with open('DependencyParsing.binaryfile', 'wb') as f:\n",
    "    pickle.dump(augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "\n",
    "# POS tagging augmenter unit test\n",
    "from data import UniversalDependenciesDataset\n",
    "dataset = UniversalDependenciesDataset(\n",
    "    # '../data/*/*/en*train*conllu',\n",
    "    # '../data/universal-dependencies-1.2/tags.txt'\n",
    "    'data/conllu/en*train*conllu',\n",
    "    'data/tags.txt'\n",
    ")\n",
    "augmenter = POSTagAugmenter(dataset)\n",
    "augmented_dataset = augmenter.augment()\n",
    "with open('POSTag.binaryfile', 'wb') as f:\n",
    "    pickle.dump(augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n",
    "\n",
    "\n",
    "from data import UniversalDependenciesDataset\n",
    "ud_dataset = UniversalDependenciesDataset(\n",
    "    # '../data/ud-treebanks-v2.6/UD_English-*/en_ewt-ud-train.conllu', \n",
    "    # '../data/ud-treebanks-v2.6/tags.txt'\n",
    "    'data/conllu/en_ewt-ud-train.conllu',\n",
    "    'data/tags.txt'\n",
    ")\n",
    "ud_rand_augmenter = POSTagRandomAugmenter(ud_dataset)\n",
    "ud_rand_augmenter.augment()\n",
    "with open('POSTagRandom.binaryfile', 'wb') as f:\n",
    "    pickle.dump(augmented_dataset , f)\n",
    "from IPython import embed; embed(using=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
